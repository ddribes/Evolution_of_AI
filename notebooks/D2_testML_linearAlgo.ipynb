{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": true,
    "id": "nhoOIJRGhkts",
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h1 style = \"text-align:center\">Analyzing a Machine Learning Model</h1> \n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "> In the vast world of Machine Learning, there are many ways to improve standard models. Whether by refining the data or adjusting the models themselves, the optimization options are numerous: feature engineering, hyperparameter tuning, choosing the right loss function, etc. These concepts are essential for any data scientist who wants to build robust and effective models. Tools such as GridSearch are fundamental for optimizing this process and developing the best possible model.\n",
    ">\n",
    "> Most practitioners follow these steps—using cross-validation or a train/test split—and iterate until they reach a good metric score. This approach is not wrong; it often leads to high-quality models. However, with a bit more effort, we can improve every model we build. This is where **error analysis** comes in.\n",
    ">\n",
    "> Error analysis is a crucial process for diagnosing the mistakes made by an ML model during training and testing. It helps data scientists and ML engineers assess model performance and identify areas for improvement. By examining errors, practitioners can gain valuable insights into data quality and relevance, problem complexity, and the effectiveness of feature engineering and model selection.\n",
    ">\n",
    "> Today, we will explore together how error analysis helps us identify error sources, evaluate and improve model performance, and keep models relevant over time. Get ready to dive into a world where every detail matters—and where a deep understanding of your machine learning models is the key to success.\n",
    ">\n",
    "> Getting good accuracy is like winning the coin toss before a match. It's nice—but there's still a lot of work left.\n",
    ">\n",
    "> The use case we will work on is a dataset about customer **churn** for a telecommunications company in California.\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h2 style = \"text-align:center\">Preliminary analysis</h2> \n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    ### Description of the 33 variables

| Name | Description |
| :--- | :--- |
| CustomerID | A unique identifier for each customer |
| Count | A value used in reports/dashboards to summarize the number of customers in a filtered set |
| Country | Customer's primary country of residence |
| State | Customer's primary state of residence |
| City | Customer's primary city of residence |
| Zip Code | Customer's primary ZIP code |
| Lat Long | Combined latitude and longitude of the customer's primary residence |
| Latitude | Latitude of the customer's primary residence |
| Longitude | Longitude of the customer's primary residence |
| Gender | Customer gender: Male, Female |
| Senior Citizen | Whether the customer is 65 or older |
| Partner | Whether the customer has a partner |
| Dependents | Whether the customer lives with dependents |
| Tenure Months | Total number of months the customer has been with the company |
| Phone Service | Whether the customer subscribes to landline phone service |
| Multiple Lines | Whether the customer subscribes to multiple phone lines |
| Internet Service | None, DSL, Fiber Optic, Cable |
| Online Security | Additional online security service |
| Online Backup | Additional online backup service |
| Device Protection | Additional device protection plan |
| Tech Support | Additional tech support plan |
| Streaming TV | Streams TV via internet |
| Streaming Movies | Streams movies via internet |
| Contract | Month-to-month, One year, Two years |
| Paperless Billing | Whether paperless billing is enabled |
| Payment Method | Bank withdrawal, Credit card, Mailed check |
| Monthly Charge | Current monthly charge |
| Total Charges | Total customer charges |
| Churn Label | Yes = customer left |
| Churn Value | 1 = left, 0 = stayed |
| Churn Score | 0–100 churn probability score |
| CLTV | Customer lifetime value |
| Churn Reason | Reason customer left |,
    "### Reading the data\n",
    "\n",
    "* Start by importing the required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CW22H5i91kt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Import the dataset and do a quick initial inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAHajESERc5E"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('Telco_customer_churn.xlsx')\n",
    "\n",
    "display(df.head())\n",
    "display(df.describe())\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Data cleaning & preprocessing\n",
    "\n",
    "* The *Total Charges* variable is not in the correct format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total Charges'] = pd.to_numeric(df['Total Charges'], errors='coerce')\n",
    "\n",
    "df['Total Charges'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Transform categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['Gender'].map({'Female':1, 'Male':0})\n",
    "\n",
    "df['Contract'] = df['Contract'].map({'Two year':2, 'One year':1, 'Month-to-month':0})\n",
    "\n",
    "for col in ['Senior Citizen', 'Partner', 'Dependents', 'Phone Service', 'Paperless Billing']:\n",
    "    df[col] = df[col].map({'Yes':1, 'No':0})\n",
    "\n",
    "df['Multiple Lines'] = df['Multiple Lines'].map({'Yes':1, 'No':0, 'No phone service':0})\n",
    "\n",
    "df['Payment Method'] = df['Payment Method'].map({'Credit card (automatic)':3, 'Bank transfer (automatic)':2, 'Mailed check':1, 'Electronic check':0})\n",
    "\n",
    "df['Internet Service'] = df['Internet Service'].map({'Fiber optic':2, 'DSL':1, 'No':0}) \n",
    "\n",
    "for col in ['Online Security', 'Online Backup', 'Device Protection', 'Tech Support', 'Streaming TV', 'Streaming Movies']:\n",
    "    df[col] = df[col].map({'Yes':1, 'No':0, 'No internet service':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### A few data visualizations\n",
    "\n",
    "* Distribution of 'Churn Reasons':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=df, x='Churn Reason', order=df['Churn Reason'].value_counts().index)\n",
    "ax.set_title('Countplot for Churn Reason')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Recode the 'Churn Reason' variable into 'Main Reason'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn Reason'].fillna('Not Applicable', inplace=True)\n",
    "\n",
    "def categorize_reason(reason):\n",
    "    if reason=='Not Applicable':\n",
    "        return np.nan\n",
    "    elif reason.split(' ')[0]=='Competitor':\n",
    "        return 'Competition'\n",
    "    elif reason.split(' ')[-1]=='dissatisfaction':\n",
    "        return 'Dissatisfaction'\n",
    "    elif (reason.split(' ')[0]=='Moved') | (reason.split(' ')[0]=='Deceased'):\n",
    "        return 'Need'\n",
    "    elif (reason.split(' ')[0]=='Price') | (reason.split(' ')[-1]=='charges'):\n",
    "        return 'Price'\n",
    "    elif (reason.split(' ')[0]=='Poor') | (reason.split(' ')[0]=='Attitude'):\n",
    "        return 'Attitude' \n",
    "    elif reason==\"Don't know\":\n",
    "        return 'Others'\n",
    "    else: \n",
    "        return 'Product'\n",
    "\n",
    "    \n",
    "    \n",
    "df['Main Reason'] = df['Churn Reason'].apply(lambda x : categorize_reason(str(x)))\n",
    "\n",
    "df['Main Reason'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* New distribution of churn reasons:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df['Main Reason'].value_counts().reset_index()\n",
    "df1.rename(columns={'index':'Main Reason', 'Main Reason':'count'}, inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "fig.suptitle('Customer Churn Rate by Demographics Segmentation', fontsize=15)\n",
    "\n",
    "demo = ['Senior Citizen', 'Partner', 'Dependents']\n",
    "\n",
    "for i, col in enumerate(demo, start=0):\n",
    "    df1 = df.groupby([col, 'Churn Label']).size().unstack()\n",
    "    df1.apply(lambda x : round((x/x.sum())*100,1), axis=1).plot(kind='barh', stacked=True, ax=ax[i])\n",
    "    ax[i].set_title('Customer Churn Rate by {}'.format(col))\n",
    "    ax[i].legend(loc='best')\n",
    "    ax[i].grid(axis='y')\n",
    "    for container in ax[i].containers:\n",
    "        ax[i].bar_label(container, label_type='center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "fig.suptitle('Customer Churn Rate by Payment and Billing Detail', fontsize=15)\n",
    "\n",
    "bill = ['Payment Method', 'Paperless Billing']\n",
    "\n",
    "for i, col in enumerate(bill, start=0):\n",
    "    df1 = df.groupby([col, 'Churn Label']).size().unstack()\n",
    "    df1.apply(lambda x : round((x/x.sum())*100,1), axis=1).plot(kind='barh', stacked=True, ax=ax[i])\n",
    "    ax[i].set_title('Customer Churn Rate by {}'.format(col))\n",
    "    ax[i].legend(loc='best')\n",
    "    ax[i].grid(axis='y')\n",
    "    for container in ax[i].containers:\n",
    "        ax[i].bar_label(container, label_type='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby(['Contract', 'Churn Label']).size().unstack()\n",
    "ax = df1.apply(lambda x : round((x/x.sum()*100),1), axis=1).plot(kind='barh', stacked=True, figsize=(5,5))\n",
    "plt.title('Customer Churn Rate by Contract')\n",
    "plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.grid(axis='x')\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Distribution of target variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=df, x='Churn Value')\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "ax.set_title('Count Plot of Churn Value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Churn Score'], kde=True)\n",
    "plt.title('Histogramme Churn Score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x='Churn Value', y ='Churn Score')\n",
    "plt.title(\"Churn Score VS Churn Value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Correlation matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "corr_matrix = df.corr('spearman', numeric_only=True)\n",
    "sns.heatmap(corr_matrix, cbar=True, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Regression model\n",
    "\n",
    "Let's start with a regression model to predict the **'Churn Score'** variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Data preparation\n",
    "\n",
    "* Remove variables that are useless or directly linked to the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df = df.drop(['CustomerID', 'Count', 'Country', 'State', 'City', 'Zip Code', 'Lat Long', 'Latitude', 'Longitude','Churn Label', 'Churn Value', 'Churn Reason', 'Main Reason'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Prepare the training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "X = reg_df.copy()\n",
    "y = X.pop('Churn Score')\n",
    "\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Train the model and compute the scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(max_depth=10, min_samples_split=10, n_estimators= 100)\n",
    "\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "predicts_rf_train = rf.predict(X_train)\n",
    "predicts_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Training data scores\")\n",
    "print(\"Random Forest model R2 : {}\".format(r2_score(y_train, predicts_rf_train)))\n",
    "print(\"MAE du modèle RF : {}\".format(mean_absolute_error(y_train, predicts_rf_train)))\n",
    "print(\"RMSE du modèle RF : {}\".format(np.sqrt(mean_squared_error(y_train, predicts_rf_train))))\n",
    "\n",
    "print(\"\\n Scores sur données de test\")\n",
    "print(\"Random Forest model R2 : {}\".format(r2_score(y_test, predicts_rf)))\n",
    "print(\"MAE du modèle RF : {}\".format(mean_absolute_error(y_test, predicts_rf)))\n",
    "print(\"RMSE du modèle RF : {}\".format(np.sqrt(mean_squared_error(y_test, predicts_rf))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> What do these scores mean? How should we interpret them?\n",
    ">\n",
    "> Metrics such as **R²** or **RMSE** are useful to compare models with each other.  \n",
    "> However, interpreting them in absolute terms to judge a model “by itself” is often harder:\n",
    ">\n",
    "> - **R²** is mainly designed for linear regression models. For non-linear models (e.g., random forests), relationships are not linear, which can make R² less intuitive or less relevant.\n",
    "> - **R²**, like **RMSE**, can be strongly influenced by outliers.\n",
    "> - A high **R²** does not always distinguish a well-fitted model from an overfitted one (overfitting) on the training data.\n",
    ">\n",
    "> *A first way to assess model usefulness is to compare it to simple baselines (“benchmark” or “naive” models):*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for a naive baseline model (mean on the training dataset)\n",
    "y_pred = np.ones(len(y_test))*y_train.mean()\n",
    "print(\"Naive baseline (mean) R2 : {}\".format(round(r2_score(y_test, y_pred),2)))\n",
    "print(\"Naive baseline (mean) MAE : {}\".format(round(mean_absolute_error(y_test, y_pred),2)))\n",
    "print(\"RMSE du modèle naïf moyenne : {}\".format(round(np.sqrt(mean_squared_error(y_test, y_pred)),2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for a naive baseline model (mean by Contract on the training dataset)\n",
    "y_pred = reg_df.loc[y_test.index,:].groupby('Contract')['Churn Score'].transform('mean')\n",
    "print(\"Naive baseline (mean) R2 contrat : {}\".format(round(r2_score(y_test, y_pred),2)))\n",
    "print(\"Naive baseline (mean) MAE contrat : {}\".format(round(mean_absolute_error(y_test, y_pred),2)))\n",
    "print(\"RMSE du modèle naïf moyenne contrat : {}\".format(round(np.sqrt(mean_squared_error(y_test, y_pred)),2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Results analysis\n",
    "\n",
    "> In online challenges, education, and research, once a model has been trained, the common tendency is to evaluate it using standard metrics and to optimize those metrics until the best possible score is reached.\n",
    ">\n",
    "> When a single metric is used to judge your model, getting an excellent score on the test set often marks the end of the optimization process.\n",
    ">\n",
    "> In practice, it is important to examine your model carefully—both the correct predictions and the errors—even if you obtain a good metric. A model can score well yet behave in a way that does not make sense for your real problem, preventing you from answering business questions or making reliable interpretations and recommendations.\n",
    ">\n",
    "> Good error analysis also helps you identify the classes and/or variables for which the model behaves incorrectly.\n",
    ">\n",
    "> Different types of errors exist:\n",
    "> - **Bias errors:** bias in the training set; the model will fail to make good predictions for certain groups.\n",
    "> - **Variance errors:** the model fails to generalize; predictions on unseen data are poor.\n",
    "> - **Noise errors:** random fluctuations in the data/model.\n",
    "> - **Labeling errors:** incorrect or inconsistent labels.\n",
    "> - **Conceptual errors:** poor generalization with many outliers or points outside the training distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Plot a scatter plot comparing predicted values to true values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=y_test,y=predicts_rf)\n",
    "g.ax.axline(xy1=(20, 20), xy2=(100,100), color=\"b\", dashes=(5, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> Possible interpretations of this plot:\n",
    ">\n",
    "> **Spread around the parity line:** The points are fairly evenly distributed around the parity line (the dotted blue diagonal), suggesting the model has reasonable predictive accuracy. However, there is substantial spread, indicating variability in prediction errors.\n",
    ">\n",
    "> **Systematic bias:** Values below ~60 tend to be systematically overestimated. Above 60, most churn scores are underestimated by the model.\n",
    ">\n",
    "> **Residual variance:** Residual variance appears relatively constant (homoscedasticity) because the spread does not change drastically across the range of values. There may be a slight increase in spread at higher values, which would require further analysis.\n",
    ">\n",
    "> **Outliers:** A few points are far from the parity line, indicating outliers or particularly large prediction errors. It is useful to analyze these cases to understand why the errors occur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Let's take a closer look at the residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "y_pred = predicts_rf\n",
    "result['y_test'] = y_test\n",
    "result['y_pred'] = y_pred\n",
    "result['residuals'] = result.y_test - result.y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = result.residuals.quantile([0.1,0.25,0.75,0.9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot\n",
    "sns.relplot(data=result, x='y_test', y='residuals', alpha=0.5, height=8, aspect=10/8)\n",
    "\n",
    "plt.plot([0,result.y_test.max()], [0,0], 'r-.,')\n",
    "\n",
    "plt.plot([0,result.y_test.max()], [quantiles[0.10],quantiles[0.10]], 'y--',\n",
    "    label=\"80% of residuals fall within this interval\")\n",
    "\n",
    "plt.plot([0,result.y_test.max()],[quantiles[0.90], quantiles[0.90]], 'y--')\n",
    "                                      \n",
    "plt.plot([0,result.y_test.max()], [quantiles[0.25], quantiles[0.25]], 'g--',\n",
    "        label=\"50% des résidus présents dans cet intervalle\")\n",
    "plt.plot([0,result.y_test.max()], [quantiles[0.75], quantiles[0.75]], 'g--')\n",
    "\n",
    "plt.xlim(0,result.y_test.max()+10)\n",
    "\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('test résidus')\n",
    "plt.title(\"Résidus\")\n",
    "plt.legend()\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Analyze the absolute residual values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = abs(y_test-predicts_rf)\n",
    "residuals.name = 'Absolute residuals'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(residuals);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pourcentage d'observations ayant une erreur inférieure à 20\n",
    "len(residuals[residuals<20])/len(residuals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The ECDF plot shows the Empirical Cumulative Distribution Function of a variable\n",
    "sns.ecdfplot(residuals)\n",
    "plt.plot([0,60],[0.7,0.7], 'y--')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* You can then compare the average error across different population groups:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is customer age related to the model's prediction difficulty?\n",
    "residuals.to_frame().join(df['Senior Citizen']).groupby('Senior Citizen').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is customer age related to the model's prediction difficulty?\n",
    "residuals.to_frame().join(df['Contract']).groupby('Contract').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Découpons le CLTV en 4 classes et comparons les erreurs pour chacune des classes\n",
    "\n",
    "reg_df['CLTV_classes'] = pd.qcut(df['CLTV'],4, labels =[1,2,3,4])\n",
    "\n",
    "residuals.to_frame().join(reg_df['CLTV_classes']).groupby('CLTV_classes').mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Classification model\n",
    "\n",
    "Let's create a binary classification model to predict the **'Churn Value'** variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Data preparation\n",
    "\n",
    "* Remove variables that are useless or directly linked to the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_df = df.drop(['CustomerID', 'Count', 'Country', 'State', 'City', 'Zip Code', 'Lat Long', 'Latitude', 'Longitude','Churn Label', 'Churn Score', 'CLTV', 'Churn Reason', 'Main Reason'], axis=1)\n",
    "\n",
    "classif_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Train the model and compute the scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "X = classif_df.copy()\n",
    "y = X.pop('Churn Value')\n",
    "\n",
    "\n",
    "# Stratified train-test split\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "for train_idx, test_idx in skfold.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    \n",
    "#StandarScaler (inutile dans le cas d'un modèle à base d'arbres)\n",
    "scaler = StandardScaler()\n",
    "X_scaled_train = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "X_scaled_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "\n",
    "# Resampling des données d'entraînement dû au déséquilibre des données\n",
    "X_scaled_train, y_train = SMOTE(sampling_strategy='minority').fit_resample(X_scaled_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.1, max_depth=6, n_estimators=100)\n",
    "xgb.fit(X_scaled_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = xgb.predict(X_scaled_test)\n",
    "print(classification_report(y_test, ypred, labels=[0,1], target_names=['Non-churned [0]', 'Churned [1]']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> What do these metrics tell us?\n",
    ">\n",
    "> How good is the model performance?\n",
    ">\n",
    "> *Plot the confusion matrix and the ROC curve.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= xgb.predict_proba(X_scaled_test)\n",
    "\n",
    "y_pred_score = preds[:,1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,5))\n",
    "ax[0].set_title('Confusion Matrix of XGBoost model:')\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, ypred, colorbar=False, cmap='crest', ax=ax[0])\n",
    "ax[0].grid(False)\n",
    "\n",
    "\n",
    "# Compute ROC metrics:\n",
    "fpr, tpr, thresholds = roc_curve(y_test.values, y_pred_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "                         \n",
    "ax[1].set_title('ROC Curve - XGBoost Classifier')\n",
    "ax[1].plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc, c='teal')\n",
    "ax[1].plot([0,1],[0,1],'--', c='darkseagreen')\n",
    "ax[1].legend(loc='lower right')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_xlabel('False Positive Rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Error analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des mauvaises prédictions\n",
    "bad_predictions = X_test[y_test!=ypred].join(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Focus on contract types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby(['Contract', 'Churn Value']).size().unstack()\n",
    "ax = df1.apply(lambda x : round((x/x.sum()*100),1), axis=1).plot(kind='barh', stacked=True)\n",
    "plt.title('Customer Churn Rate by Contract')\n",
    "plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.grid(axis='x')\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = bad_predictions.groupby(['Contract', 'Churn Value']).size().unstack()\n",
    "ax = df1.apply(lambda x : round((x/x.sum()*100),1), axis=1).plot(kind='barh', stacked=True, figsize=(5,5))\n",
    "plt.title('Customer Churn Rate by Contract')\n",
    "plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.grid(axis='x')\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> Churners on month-to-month contracts are harder for the model to identify.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Let's examine differences by gender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Gender'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_predictions['Gender'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Show the proportions in the test set and in the misclassified predictions only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "for col in X_test.loc[:,X_test.nunique()<5].columns:\n",
    "    # Calcul des proportions dans X_test\n",
    "    vc_test = X_test[col].value_counts(normalize=True)\n",
    "    \n",
    "    # Calcul des proportions dans les mauvaises prédictions\n",
    "    vc_bad_pred = bad_predictions[col].value_counts(normalize=True)\n",
    "    \n",
    "    # Préparation des résultats pour la variable courante\n",
    "    result = pd.DataFrame({\n",
    "        f'{col}_X_test': vc_test,\n",
    "        f'{col}_bad_predictions': vc_bad_pred\n",
    "    })\n",
    "\n",
    "    display(pd.concat([results, result]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Let's examine accuracy differences for each categorical variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_with_predictions = X_test.copy()\n",
    "X_test_with_predictions['TrueValue'] = y_test\n",
    "X_test_with_predictions['Prediction'] = ypred\n",
    "\n",
    "# Select categorical columns\n",
    "categorical_columns = X_test.loc[:,X_test.nunique()<5].columns\n",
    "\n",
    "# Function to compute rates\n",
    "def calculate_accuracy_rates(df, col, true_col, pred_col):\n",
    "    # Calculer le taux de bonnes et de mauvaises prédictions pour chaque catégorie\n",
    "    accuracy_rates = df.groupby(col).apply(\n",
    "        lambda x: pd.Series({\n",
    "            'Accuracy': (x[true_col] == x[pred_col]).mean(),\n",
    "        })\n",
    "    )\n",
    "    return accuracy_rates\n",
    "\n",
    "# Calculer et afficher les taux pour chaque variable catégorielle\n",
    "for col in categorical_columns:\n",
    "    rates = calculate_accuracy_rates(X_test_with_predictions, col, 'TrueValue', 'Prediction')\n",
    "    print(f\"Taux pour la variable '{col}':\\n{rates}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* It is also useful to analyze low-confidence predictions to identify examples where the model struggles to decide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[(y_pred_score>0.5) & (y_pred_score<0.65) & (y_test==1)].join(df['Main Reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Lift curve / Cumulative gains\n",
    "\n",
    "The lift curve helps you select a percentage of observations with the highest probability of belonging to the class of interest, and shows how many elements of that class you can capture.\n",
    "\n",
    "For example, in the curve below, we can see that by selecting the top 20% of observations by predicted probability, more than 50% of churners are included. In theory, selecting 20% of observations at random would capture only 20% of churners—so the model increases the chance of targeting a churner by about 2.5×.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_cumulative_gain(y_test, preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> To go further, you can compute model precision as a function of the chosen probability threshold.\n",
    ">\n",
    "> If the model behaves as expected, increasing the probability threshold increases precision (but decreases the sample size). Depending on your goals, this can be very useful information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seuil = 0.8 #ou 0.7, 0.85, 0.9 etc..\n",
    "\n",
    "predictions = np.where(y_pred_score > 0.8, 0, 1)\n",
    "\n",
    "pd.Series(predictions).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> To evaluate model relevance and incorporate business constraints, you can also create your own evaluation metric.\n",
    ">\n",
    "> Here, we have a score that represents the “value” of a customer for the company, so we can define a penalty function that weights model errors by customer value (CLTV).\n",
    ">\n",
    "> Example:\n",
    ">\n",
    "> $ penalty = \\sum _{i=1}^{4999} penalty_i $\n",
    ">\n",
    "> where $penalty_i$ is computed as:\n",
    ">\n",
    ">$\n",
    "penalty_i = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        0 & \\mbox{if  } y_i = \\hat{y}_i \\\\ \n",
    "        1 & \\mbox{if  } \\hat{y}_i = 1 \\mbox{ and }  y_i = 0 \\\\\n",
    "        CLTV_i & \\mbox{if  } \\hat{y}_i = 0  \\mbox{ and } y_i = 1 \\\\\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$\n",
    ">\n",
    "> More generally, don’t hesitate to create metrics that compute the “cost” (financial or human, for example) of a model based on its predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty(preds):\n",
    "    \n",
    "    sum = []\n",
    "    \n",
    "    for i in range(len(preds)):\n",
    "        if preds[i]-y_test.iloc[i] == -1:\n",
    "            sum.append(df.loc[y_test.index, 'CLTV'].iloc[i])\n",
    "            \n",
    "        \n",
    "    else: sum.append(preds[i]-y_test.iloc[i])\n",
    "    \n",
    "    return(sum)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
